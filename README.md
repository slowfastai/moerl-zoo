<div align="center">

<a href="https://github.com/slowfastai/moerl">
  <img alt="moerl logo" src="https://raw.githubusercontent.com/slowfastai/moerl/main/docs/assets/logo.png" height="100">
</a>

<p>
  <b>Custom patches and utilities for <a href="https://github.com/slowfastai/moerl">MoERL</a> — an efficient RL fine-tuning framework for Mixture of Experts (MoE) LLMs.</b>
</p>

</div>

---

## ✨ What is moerl-zoo?

**moerl-zoo** is a toolkit inspired by [unsloth-zoo](https://github.com/unslothai/unsloth-zoo), designed to serve the needs of [MoERL](https://github.com/slowfastai/moerl). It includes lightweight patches, wrappers, and utilities specifically adapted for MoE reinforcement learning fine-tuning pipelines.

> 🦥 This project is based on `unsloth-zoo`, created by Daniel Han-Chen and the Unsloth team. All modifications are made under the terms of the **LGPL-3.0-or-later** license.

## 🚀 Features

- 🧩 Custom monkey patches and related utilities designed for MoERL compatibility
- 🚀 Supports efficient MoE-based RL fine-tuning  
- ⚡ Fully compatible with `vllm`, `bitsandbytes`, `transformers`, `trl`, and more  

## 📦 Installation

```bash
pip install "moerl_zoo @ git+https://github.com/slowfastai/moerl-zoo.git"
```
## 🤝 Acknowledgements

Huge thanks to the Unsloth project for their foundational work in efficient LLM optimization ❤️. MoERL builds upon their vision while tailoring functionality toward MoE RL.

## 📜 License

This project is licensed under the **GNU Lesser General Public License v3.0 or later (LGPL-3.0-or-later)**.

The original base project, 🦥 [unsloth-zoo](https://github.com/unslothai/unsloth-zoo), is licensed under the **GNU Lesser General Public License v3.0 or later (LGPL-3.0-or-later)**.


All original LICENSE and NOTICE files have been preserved where applicable.